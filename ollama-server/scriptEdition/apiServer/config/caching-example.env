# LLM Provider Configuration
LLM_PROVIDER=anthropic
ANTHROPIC_API_KEY=your_anthropic_api_key_here
ANTHROPIC_MODEL=claude-3-5-haiku-20241022
# ANTHROPIC_BASE_URL=https://api.anthropic.com/v1  # Optional: override default URL

# Alternative OpenAI Configuration
# LLM_PROVIDER=openai
# OPENAI_API_KEY=your_openai_api_key_here
# OPENAI_MODEL=gpt-4-turbo-preview
# OPENAI_BASE_URL=https://api.openai.com/v1  # Optional: override default URL

# Custom/Proxy URLs Examples:
# ANTHROPIC_BASE_URL=https://your-proxy.com/anthropic/v1
# OPENAI_BASE_URL=https://your-proxy.com/openai/v1
# ANTHROPIC_BASE_URL=http://localhost:8080/v1  # Local development

# Caching is now controlled per-request via the API
# Example API requests:

# With caching enabled (default):
# POST /analyze
# {
#   "question": "What is the current price of AAPL?",
#   "model": "claude-3-5-haiku-20241022",
#   "enable_caching": true
# }

# With caching disabled:
# POST /analyze
# {
#   "question": "What is the current price of AAPL?", 
#   "enable_caching": false
# }